#!/bin/sh
#SBATCH -p cpl
#SBATCH --time=7-0    # -- first number is days requested, second number is hours
#SBATCH --gres=gpu:titan-x:2  #  -- request the number of gpus.
#SBATCH -c 1                  #  -- request number of cores
#SBATCH --mem=8G              #  -- request memory
#SBATCH --mail-user=jennhu@mit.edu # -- use this to send an automated email when:
#SBATCH --mail-type=end            # -- your job completes successfully
#SBATCH --mail-type=fail           # -- or if your job fails

ROOT="/om/group/cpl/language-models/transformer-xl"
MODEL_DIR="/om/group/cpl/language-models/syntaxgym/models/${MODEL_KEY}/${MODEL_KEY}_${CORPUS}_${SEED}"
mkdir -p $MODEL_DIR
DATA="/om/group/cpl/language-models/syntaxgym/data/${CORPUS}/unk"

# Make sure data is tokenized and unkified!
source activate transXL

# Adapted from https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/run_enwik8_base.sh
cd $ROOT
python train.py \
    --cuda \
    --data ${DATA} \
    --seed ${SEED} \
    --work_dir ${MODEL_DIR} \
    --log ${LOGS}/${MODEL_KEY}_${CORPUS}_${SEED}.txt \
    --dataset ptb \
    --n_layer 12 \
    --d_model 512 \
    --n_head 8 \
    --d_head 64 \
    --d_inner 2048 \
    --dropout 0.1 \
    --dropatt 0.0 \
    --optim adam \
    --lr 0.00025 \
    --warmup_step 0 \
    --max_step 400000 \
    --tgt_len 512 \
    --mem_len 512 \
    --eval_tgt_len 128 \
    --batch_size 22 \
    --multi_gpu \
    --gpu0_bsz 4