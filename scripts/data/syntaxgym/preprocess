#!/bin/bash
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Author: Jennifer Hu
# Date: 2019-10-25
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Usage: ./preprocess <CORPUS_NAME> [--test]
# Function: preprocesses raw corpus (tokenization and unkification)
#
# Expects the following directory structure:
#
# <CORPUS_NAME>/
# ├── raw/
# │   ├── train.txt
# │   └── valid.txt
#
# Will create the following subdirectories within <CORPUS_NAME>:
# ├── token/
# │   ├── train.txt
# │   └── valid.txt
# ├── unk/
# │   ├── train.txt
# │   └── valid.txt
#
# The vocabulary dictionaries (token --> frequency) will also be saved under
# <CORPUS_NAME>/train_vocab.pkl and <CORPUS_NAME>/valid_vocab.pkl.
# Pass the --test flag to additionally tokenize and unkify a test file.
# The files within <CORPUS_NAME>/unk are ready to be fed to models for training.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Set variables.
MODEL="rnng"
IMAGE="cpllab/language-models:${MODEL}"
n=1
CORPUS=$1
RAW="$CORPUS/raw"
TOKEN="$CORPUS/token"
UNK="$CORPUS/unk"

# Make directories for tokenized and unkified data.
mkdir -p $TOKEN $UNK

# Tokenize training file.
echo "Tokenizing training data --> $TOKEN/train.txt"
docker run --rm -i ${IMAGE} tokenize /dev/stdin < $RAW/train.txt > $TOKEN/train.txt

# Unkify training file.
echo "Unkifying training data --> $UNK/train.txt"
python fine-grained-unkify.py $TOKEN/train.txt $TOKEN/train.txt \
    --save_dict $CORPUS/train_vocab.pkl \
    --brackets \
    --n $n \
    > $UNK/train.txt

# Tokenize dev file.
echo "Tokenizing dev data --> $TOKEN/valid.txt"
docker run --rm -i ${IMAGE} tokenize /dev/stdin < $RAW/valid.txt > $TOKEN/valid.txt

# Unkify dev file.
echo "Unkifying dev data --> $UNK/valid.txt"
python fine-grained-unkify.py $TOKEN/train.txt $TOKEN/valid.txt \
    --save_dict $CORPUS/valid_vocab.pkl \
    --brackets \
    --n $n \
    > $UNK/valid.txt

# Check for test flag. Should only be used for base PTB corpus.
if echo $* | grep -e "--test" -q
then
    # Tokenize test file.
    echo "Tokenizing test data --> $TOKEN/test.txt"
    docker run --rm -i ${IMAGE} tokenize /dev/stdin < $RAW/test.txt > $TOKEN/test.txt

    # Unkify test file.
    echo "Unkifying test data --> $UNK/test.txt"
    python fine-grained-unkify.py $TOKEN/train.txt $TOKEN/test.txt \
        --save_dict $CORPUS/test_vocab.pkl \
        --brackets \
        --n $n \
        > $UNK/test.txt
fi