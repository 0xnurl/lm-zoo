#!/usr/bin/env python

import json
import os
import sys

import torch

from transformers import AutoTokenizer

# Load spec template.
with open("/opt/spec.json", "r") as spec_f:
    spec = json.load(spec_f)

# Load tokenizer.
model_path = os.environ["TRANSFORMER_MODEL_PATH"]
tokenizer = AutoTokenizer.from_pretrained(model_path)

def filter_none(xs):
    return [x for x in xs if x is not None]

# Set spec vocabulary information from tokenizer.
special_tokens = set(tokenizer.all_special_tokens) - \
        {tokenizer.bos_token, tokenizer.eos_token, tokenizer.unk_token}

spec["vocabulary"] = {
    "items": list(tokenizer.get_vocab().keys()),

    "prefix_types": filter_none([tokenizer.bos_token]),
    "suffix_types": filter_none([tokenizer.eos_token]),
    "unk_types": filter_none([tokenizer.unk_token]),
    "special_types": list(special_tokens),
}

json.dump(spec, sys.stdout)
